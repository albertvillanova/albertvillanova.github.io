{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning from Supervised and Self-Supervised Pretraining using PyTorch Lightning\n",
    "\n",
    "- toc: true\n",
    "- category: blog\n",
    "- tags: pytorch-lightning transfer-learning supervised-learning self-supervised-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit to original author William Falcon, and also to Alfredo Canziani for posting the video presentation: [_Supervised and self-supervised transfer learning (with PyTorch Lightning)_](https://www.youtube.com/watch?v=nCq_vy9qE-k)\n",
    "\n",
    "In the video presentation, they compare transfer learning from pretrained:\n",
    "* supervised\n",
    "* self-supervised\n",
    "\n",
    "However, I would like to point out that the comparison is not entirely fair for the case of supervised pretraining. The reason is that they do not replace the last fully-connected layer of the supervised pretrained backbone model with the new finetuning layer. Instead, they stack the new finetuning layer on top of the pretrained model (including its last fully connected layer).\n",
    "\n",
    "This is a clear disadvantage for the supervised pretrained model because:\n",
    "* all its expressive power is contained in the output of the penultimate layer\n",
    "* and it was already used by the last fully-connected layer to predict 1,000 classes\n",
    "\n",
    "When stacking the finetuning layer on top of it, this has to perform the 10-class classification using the output of the 1,000-class classfication layer.\n",
    "\n",
    "On the contrary, if we replace the backbone last fully connected layer with the new finetuning layer, it will be able to perform the 10-class classification using all the expressive power of the features coming from the output of the penultimate layer.\n",
    "\n",
    "In this notebook I show that if we replace the last fully connected layer with the new finetuning layer, both supervised and self-supervised approaches give comparable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pytorch-lightning\n",
    "!pip install pytorch-lightning-bolts==0.2.5rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl version: 1.0.6\n",
      "pl_bolts version: 0.2.5rc1\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import pl_bolts\n",
    "\n",
    "print(f\"pl version: {pl.__version__}\")\n",
    "print(f\"pl_bolts version: {pl_bolts.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31534b5987ec499380fc4a7c83719ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "resnet50 = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0cf0725f414a31abb0ae90bfd0285c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./cifar-10-python.tar.gz to .\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                                 std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "cf10_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "cifar_10 = CIFAR10('.', train=True, download=True, transform=cf10_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: 6\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image, label = next(iter(cifar_10))\n",
    "print(f\"LABEL: {label}\")\n",
    "plt_img = image.numpy().transpose(1, 2, 0)\n",
    "plt.imshow(plt_img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(cifar_10, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32, 32]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    x, y = batch\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_classes = 10\n",
    "resnet50.fc = torch.nn.Linear(resnet50.fc.in_features, num_classes)\n",
    "\n",
    "# Use afterwards in optimizer: resnet50.fc.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3801, -0.5696,  0.6141, -0.0555, -0.0337, -0.0064,  0.6410,  0.5071,\n",
       "          0.5621, -0.3443],\n",
       "        [ 0.8296, -0.8653,  0.4210, -0.1691,  0.4789, -0.6263,  1.6198,  0.1962,\n",
       "          1.6246, -0.7211],\n",
       "        [-0.2604,  0.1539,  0.4095, -0.1340, -0.0187, -0.2622,  0.5658,  0.2214,\n",
       "          0.6504, -0.0676],\n",
       "        [ 0.2170,  0.0293,  0.1628,  0.3194, -0.0261, -0.3643,  0.0667,  0.0840,\n",
       "          0.2994, -0.4926],\n",
       "        [ 0.5270, -0.7288, -0.4667,  0.3634,  0.4815,  0.3479,  1.4035,  0.4295,\n",
       "          0.3564, -0.1770]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "\n",
    "preds = resnet50(x)\n",
    "preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0569, 0.0470, 0.1537, 0.0787, 0.0804, 0.0826, 0.1579, 0.1381, 0.1459,\n",
       "         0.0589],\n",
       "        [0.1203, 0.0221, 0.0799, 0.0443, 0.0847, 0.0280, 0.2650, 0.0638, 0.2663,\n",
       "         0.0255],\n",
       "        [0.0646, 0.0978, 0.1263, 0.0733, 0.0823, 0.0645, 0.1476, 0.1046, 0.1606,\n",
       "         0.0784],\n",
       "        [0.1171, 0.0970, 0.1109, 0.1297, 0.0918, 0.0655, 0.1007, 0.1025, 0.1271,\n",
       "         0.0576],\n",
       "        [0.1118, 0.0318, 0.0414, 0.0949, 0.1068, 0.0935, 0.2687, 0.1014, 0.0943,\n",
       "         0.0553]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "preds = softmax(preds, dim=-1)\n",
    "preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 8, 8, 3, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels = torch.argmax(preds, dim=-1)\n",
    "pred_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 9, 1, 8, 8])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Bolts: Data Module: 3 data loaders\n",
    "\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "\n",
    "dm = CIFAR10DataModule('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting only the new finetuning layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PyTorch\n",
    "\n",
    "# from torch.nn.functional import cross_entropy\n",
    "# from torch.optim import Adam\n",
    "\n",
    "# optimizer = Adam(resnet50.fc.parameters(), lr=1e-3)\n",
    "\n",
    "# epochs = 10\n",
    "# for epoch in range(epochs):\n",
    "#     for batch in dm.train_dataloader():\n",
    "#         x, y = batch\n",
    "\n",
    "#         # features = backbone(x)\n",
    "#         # # disable gradients to backbone if all parameters used by the optimizer\n",
    "#         # features = features.detach()\n",
    "\n",
    "#         # # tell PyTorch not to track the computational graph: much faster, less memory used: not backpropagated\n",
    "#         # with torch.no_grad():\n",
    "#         #     features = backbone(x)\n",
    "\n",
    "#         # preds = finetune_layer(features)\n",
    "\n",
    "#         preds = resnet50(x)\n",
    "\n",
    "#         loss = cross_entropy(preds, y)\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.functional import accuracy\n",
    "\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.optim import Adam\n",
    "\n",
    "class ImageClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes=10, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # self.num_classes = num_classes\n",
    "        # self.lr = lr\n",
    "\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.model.fc = torch.nn.Linear(self.model.fc.in_features, num_classes)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # return the loss given a batch: this has a computational graph attached to it: optimization\n",
    "        x, y = batch\n",
    "        preds = self.model(x)\n",
    "        loss = cross_entropy(preds, y)\n",
    "        self.log('train_loss', loss)  # lightning detaches your loss graph and uses its value\n",
    "        self.log('train_acc', accuracy(preds, y))\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # return optimizer\n",
    "        optimizer = Adam(self.model.fc.parameters(), lr=self.hparams.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: you passed in a val_dataloader but have no validation_step. Skipping validation loop\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 23 M  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea32acfe4c64b47b94822fdafe480bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = ImageClassifier()\n",
    "\n",
    "trainer = pl.Trainer(progress_bar_refresh_rate=20, gpus=1, max_epochs=2)  # for Colab: set refresh rate to 20 instead of 10 to avoid freezing\n",
    "trainer.fit(classifier, dm)  # train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        (async () => {\n",
       "            const url = await google.colab.kernel.proxyPort(6006, {\"cache\": true});\n",
       "            const iframe = document.createElement('iframe');\n",
       "            iframe.src = url;\n",
       "            iframe.setAttribute('width', '100%');\n",
       "            iframe.setAttribute('height', '800');\n",
       "            iframe.setAttribute('frameborder', 0);\n",
       "            document.body.appendChild(iframe);\n",
       "        })();\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/20201117-1-train_acc.svg \"Train Accuracy\")\n",
    "\n",
    "![](img/20201117-1-train_loss.svg \"Train Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting all the model after 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.functional import accuracy\n",
    "\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.optim import Adam\n",
    "\n",
    "class ImageClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes=10, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # self.num_classes = num_classes\n",
    "        # self.lr = lr\n",
    "\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.model.fc = torch.nn.Linear(self.model.fc.in_features, num_classes)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # return the loss given a batch: this has a computational graph attached to it: optimization\n",
    "        x, y = batch\n",
    "        if self.trainer.current_epoch == 10:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = True\n",
    "        preds = self.model(x)\n",
    "        loss = cross_entropy(preds, y)\n",
    "        self.log('train_loss', loss)  # lightning detaches your loss graph and uses its value\n",
    "        self.log('train_acc', accuracy(preds, y))\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # return optimizer\n",
    "        optimizer = Adam(self.model.parameters(), lr=self.hparams.lr)  # self.model.fc.parameters()\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: you passed in a val_dataloader but have no validation_step. Skipping validation loop\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 23 M  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a40660845444a5bb03756476286750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = ImageClassifier()\n",
    "\n",
    "trainer = pl.Trainer(progress_bar_refresh_rate=5, gpus=1, limit_train_batches=20, max_epochs=20)\n",
    "trainer.fit(classifier, dm)  # train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 327), started 0:01:19 ago. (Use '!kill 327' to kill it.)"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        (async () => {\n",
       "            const url = await google.colab.kernel.proxyPort(6006, {\"cache\": true});\n",
       "            const iframe = document.createElement('iframe');\n",
       "            iframe.src = url;\n",
       "            iframe.setAttribute('width', '100%');\n",
       "            iframe.setAttribute('height', '800');\n",
       "            iframe.setAttribute('frameborder', 0);\n",
       "            document.body.appendChild(iframe);\n",
       "        })();\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/20201117-2-train_acc.svg \"Train Accuracy\")\n",
    "\n",
    "![](img/20201117-2-train_loss.svg \"Train Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Supervised Pretraining\n",
    "\n",
    "https://pytorch-lightning-bolts.readthedocs.io/en/latest/self_supervised_models.html#swav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting all the model after 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://pl-bolts-weights.s3.us-east-2.amazonaws.com/swav/swav_imagenet/swav_imagenet.pth.tar\" to /root/.cache/torch/hub/checkpoints/swav_imagenet.pth.tar\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09018da0592343f982be0023aeb55072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=338149121.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.functional import accuracy\n",
    "\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.optim import Adam\n",
    "\n",
    "from pl_bolts.models.self_supervised import SwAV\n",
    "weight_path = 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/swav/swav_imagenet/swav_imagenet.pth.tar'\n",
    "swav = SwAV.load_from_checkpoint(weight_path, strict=True)\n",
    "\n",
    "# from pl_bolts.models.self_supervised import SimCLR\n",
    "# weight_path = 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/simclr/simclr-cifar10-v1-exp12_87_52/epoch%3D960.ckpt'\n",
    "# simclr = SimCLR.load_from_checkpoint(weight_path, strict=False)\n",
    "\n",
    "\n",
    "class ImageClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes=10, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # self.num_classes = num_classes\n",
    "        # self.lr = lr\n",
    "\n",
    "        # self.model = models.resnet50(pretrained=True)\n",
    "        self.backbone = swav.model\n",
    "        # self.backbone = simclr\n",
    "\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # self.model.fc = torch.nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        self.finetune_layer = torch.nn.Linear(3000, num_classes)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # return the loss given a batch: this has a computational graph attached to it: optimization\n",
    "        x, y = batch\n",
    "        if self.trainer.current_epoch == 10:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = True\n",
    "        (features1, features2) = self.backbone(x)\n",
    "        features = features2\n",
    "        # features = self.backbone(x)\n",
    "        preds = self.finetune_layer(features)\n",
    "        loss = cross_entropy(preds, y)\n",
    "        self.log('train_loss', loss)  # lightning detaches your loss graph and uses its value\n",
    "        self.log('train_acc', accuracy(preds, y))\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # return optimizer\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.lr)  # self.model.fc.parameters()\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: you passed in a val_dataloader but have no validation_step. Skipping validation loop\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name           | Type   | Params\n",
      "------------------------------------------\n",
      "0 | backbone       | ResNet | 28 M  \n",
      "1 | finetune_layer | Linear | 30 K  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f3adea67544beba3e07417703ca606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = ImageClassifier()\n",
    "\n",
    "trainer = pl.Trainer(progress_bar_refresh_rate=5, gpus=1, limit_train_batches=20, max_epochs=20)\n",
    "trainer.fit(classifier, dm)  # train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 327), started 0:03:19 ago. (Use '!kill 327' to kill it.)"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        (async () => {\n",
       "            const url = await google.colab.kernel.proxyPort(6006, {\"cache\": true});\n",
       "            const iframe = document.createElement('iframe');\n",
       "            iframe.src = url;\n",
       "            iframe.setAttribute('width', '100%');\n",
       "            iframe.setAttribute('height', '800');\n",
       "            iframe.setAttribute('frameborder', 0);\n",
       "            document.body.appendChild(iframe);\n",
       "        })();\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/20201117-3-train_acc.svg \"Train Accuracy\")\n",
    "\n",
    "![](img/20201117-3-train_loss.svg \"Train Loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
